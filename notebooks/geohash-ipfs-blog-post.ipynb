{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbaba21c-e681-4387-b325-1ff824cd322f",
   "metadata": {},
   "source": [
    "# Proof of concept for querying spatial data on IPFS using geohash\n",
    "\n",
    "InterPlanetary File System (IPFS) offers an alternative approach to store and distribute spatial data. Size of spatial data grows exponentially so it is imperative to perform efficient partial selection of data to avoid large bulk download over the IPFS which can save both transaction time and local storage space. Yet there is a lack of support in spatial query functionality like PostGIS in the traditional database of PostgreSQL. This blog post introduces one type of spatial indexing â€“ geohash and walks through a proof of concept to integrate geohash into the IPFS system. And the final section depicts a preliminary result for the implementation in comparison with solutions using local storage, local database, and vanilla IPFS without any spatial index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed18e8-0d31-417b-a648-75c2feb4b591",
   "metadata": {},
   "source": [
    "## Geohash\n",
    "> Geohash is a public domain geocode system invented in 2008 by Gustavo Niemeyer which encodes a geographic location into a short string of letters and digits. Similar ideas were introduced by G.M. Morton in 1966. It is a hierarchical spatial data structure which subdivides space into buckets of grid shape, which is one of the many applications of what is known as a Z-order curve, and generally space-filling curves.\n",
    "\n",
    "Compared to other prevalent hierarchical spatial data structure like R-tree and quadtree, geohash is using an absolute coordinate-encoding mapping which does not change value by the extent of dataset. \n",
    "\n",
    "![The 6g cell and its sub-grid](https://upload.wikimedia.org/wikipedia/commons/3/3d/Geohash-grid.png)\n",
    "\n",
    "Following code snippets show basic usage of the Python package pygeohash to encode a coordinates to geohash and to decode a geohash to the centeriod coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5a93c-3cc1-4663-bfc4-0003970131d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygeohash (https://github.com/wdm0006/pygeohash)\n",
    "import pygeohash as pgh\n",
    "\n",
    "pgh.encode(latitude=42.6, longitude=-5.6)\n",
    "# >>> 'ezs42e44yx96'\n",
    "\n",
    "pgh.encode(latitude=42.6, longitude=-5.6, precision=5)\n",
    "# >>> 'ezs42'\n",
    "\n",
    "pgh.decode(geohash='ezs42')\n",
    "# >>> ('42.6', '-5.6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba503812-40e7-4f36-be85-1c9fcc612b3d",
   "metadata": {},
   "source": [
    "## Proof of concept: Mapping geohash with spatial asset via index folder\n",
    "![geohash-aux-folder](../assets/geohash_aux_folder.png)\n",
    "\n",
    "The most straightforward solution is to create a hierarchical folder structure with subfolder names representing the geohash subgrids within the parent grid. The leaf node folder refers to geohash with the finest resolution, and contains CIDs that can point to the asset located in the leaf geohash. The entire folder would be packed and uploaded to IPFS as a separate asset. The query process consists of two steps: locate the target geohash; and recursively collect all the CIDs under the geohash node. Using the CIDs from the query steps, a partial retrieval can be achieved. Here is an example of 2600 restaurant points in Washington, DC, downloaded from OpenStreetMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f839ad0-2903-47e5-8adb-cb2d2518dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load geojson\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05106e-b129-4e15-82ef-f0ee4545c00c",
   "metadata": {},
   "source": [
    "### load point data and assign geohash to each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99591842-333d-4df5-9c80-2c610c1c98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_points = gpd.read_file(f\"../../data/maryland_demo/dc_restaurants.geojson\")\n",
    "\n",
    "demo_points = pd.concat([demo_points,demo_points.get_coordinates()],axis=1)\n",
    "demo_points['geohash'] = demo_points.apply(lambda row: pgh.encode(row['y'], row['x'],precision=6), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089817f6-bd91-4506-bbe3-37694e947e5e",
   "metadata": {},
   "source": [
    "### \"shatter\" the points as individual geojson and calculate CIDs\n",
    "NOTE: make sure IPFS daemon is running before calculating CIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09e656-ffe4-4dbf-96dc-4b66668f0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the individual GeoJSON files will be saved\n",
    "asset = \"dc_restaurants\"\n",
    "directory = f\"../data/geohash_{asset}\"\n",
    "# Make sure the directory exists, if not create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Initialize an empty list to store file paths\n",
    "file_paths = []\n",
    "# Loop through each row in GeoDataFrame\n",
    "for index, row in demo_points.iterrows():\n",
    "    # Slice the GeoDataFrame to get a single feature (row)\n",
    "    single_feature_gdf = demo_points.iloc[[index]]\n",
    "\n",
    "    # Get 'osm_id' for the single feature\n",
    "    osm_id = row['osm_id']\n",
    "\n",
    "    # Define the full file path\n",
    "    file_path = os.path.join(directory, f\"{osm_id}.geojson\")\n",
    "\n",
    "    # Save single feature GeoDataFrame as GeoJSON\n",
    "    single_feature_gdf.to_file(file_path, driver=\"GeoJSON\")\n",
    "\n",
    "    # Append file_path to list\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Create a new column in the original GeoDataFrame to store file paths\n",
    "demo_points['single_path'] = file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596a145-dfe9-48b8-9b1b-de168339bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure ipfs daemon is running!!\n",
    "def compute_cid(file_path):\n",
    "    import subprocess\n",
    "    cid = subprocess.check_output([\"ipfs\", \"add\", \"-qn\", file_path]).decode().strip()\n",
    "    return cid\n",
    "\n",
    "demo_points['single_cid'] = demo_points.apply(lambda x: compute_cid(x['single_path']),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f5b24-cab7-41eb-b3ac-c5257bb7c264",
   "metadata": {},
   "source": [
    "### create index folder with a trie data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ebc0a-fa48-4f9c-8fba-ab4859e1eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.value = []\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, index, value):\n",
    "        node = self.root\n",
    "        for char in str(index):\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.value.append(value)\n",
    "\n",
    "    def get(self, index):\n",
    "        node = self.root\n",
    "        for char in str(index):\n",
    "            if char not in node.children:\n",
    "                return None\n",
    "            node = node.children[char]\n",
    "        return node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c6b29-f215-4fb4-8571-2dc013045c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrange cid by geohash\n",
    "pairs = list(zip(demo_points['geohash'],demo_points['single_cid']))\n",
    "# Create an empty Trie dictionary\n",
    "trie_dict = Trie()\n",
    "\n",
    "# Insert each index-value pair into the Trie dictionary\n",
    "for index, value in pairs:\n",
    "    trie_dict.insert(index, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d3fc9-162c-47a1-a846-36de9fb8d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_trie(trie_node,geohash,root_path):\n",
    "    #export geojson at current hash level\n",
    "    next_path = root_path+\"/\"+\"\".join(geohash)\n",
    "    leaf_path = root_path+f\"/{geohash}.txt\"\n",
    "    print(geohash,root_path,next_path,leaf_path)\n",
    "    if trie_node.value:\n",
    "        # Open a file in write mode\n",
    "        with open(leaf_path, 'w') as f:\n",
    "            for item in trie_node.value:\n",
    "                f.write(f\"{item}\\n\")\n",
    "    #make path and export to sub folder\n",
    "    import os \n",
    "    if trie_node.children and not os.path.exists(next_path):\n",
    "        os.makedirs(next_path)\n",
    "    for ch in trie_node.children:\n",
    "        child_hash = geohash+ch\n",
    "        export_trie(trie_node.children[ch],child_hash,next_path)\n",
    "\n",
    "export_trie(trie_dict.root,\"\",f\"../data/geohash_{asset}/index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2add1db-d3a6-4320-a51d-acebb7048dff",
   "metadata": {},
   "source": [
    "## Performance benchmarking: query queen neighbors from a given geohash\n",
    "\n",
    "Example query to retrieve 329 out of 2675 point features: the target is to get nearby restaurants (green dots) from the neighboring geohash (blue grids) of a selected standing point (red dot) within a geohash grid (_dqcjy_)\n",
    "\n",
    "![query_dc](../assets/query_dc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13820894-43ad-40fd-a44c-035bbc235af6",
   "metadata": {},
   "source": [
    "### Calculate queen neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb98be-0250-4cd1-816e-afcbc1fc0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queen_neighbors(geohash: str) -> list:\n",
    "    import pygeohash as pgh\n",
    "    nei = rook_neighbors(geohash)\n",
    "    directions = [\"right\",\"bottom\",\"left\",\"top\"]\n",
    "    for i in range(4):\n",
    "        nei.append(pgh.get_adjacent(nei[i],directions[i]))\n",
    "    return nei\n",
    "q_geohash = \"dqcjy\"\n",
    "qn = queen_neighbors(q_geohash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a0e21-30d8-417e-9cf7-d507efa50992",
   "metadata": {},
   "source": [
    "### Query the cid list within the queen neighbor geohashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3061c51d-643d-4c7e-a156-4c2fb8819b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index structure\n",
    "\n",
    "## from file structure\n",
    "def compose_path(s,root):\n",
    "    \"\"\"\n",
    "    compose path a/ab/abc for geohash `abc`\n",
    "    \"\"\"\n",
    "    path = [root]\n",
    "    for i in range(len(s)):\n",
    "        path.append(s[:i+1])\n",
    "    return \"/\".join(path)\n",
    "\n",
    "def process_leaf_node(leaf):\n",
    "    \"\"\"\n",
    "    process index leaf.\n",
    "    leaf: txt file path of a index leaf, like a//ab/abc.txt\n",
    "    \"\"\"\n",
    "    with open(leaf, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "def traverse_sub_node(node):\n",
    "    \"\"\"\n",
    "    recursively collect all the leaf node under the current node\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    results=[]\n",
    "    excludes = [\".ipynb_checkpoints\"]\n",
    "    # Get list of items in the directory\n",
    "    subfolders = [d for d in os.listdir(node) if os.path.isdir(os.path.join(node, d)) and d not in excludes]\n",
    "    # If there are subfolders, traverse them\n",
    "    if subfolders:\n",
    "        for subfolder in subfolders:\n",
    "            results.extend(traverse_sub_node(os.path.join(node, subfolder)))\n",
    "    else:\n",
    "        # Otherwise, process txt files in the directory\n",
    "        txt_files = [f for f in os.listdir(node) if f.endswith('.txt')]\n",
    "        for txt_file in txt_files:\n",
    "            results.extend(process_leaf_node(os.path.join(node, txt_file)))\n",
    "    return results\n",
    "def query_feature_cid_by_geohash(geohash: str, index_root: str) -> list:\n",
    "    \"\"\"\n",
    "    find matching geohash or sub-level hashs\n",
    "    \"\"\"\n",
    "    import os\n",
    "    target_path = compose_path(geohash,index_root)\n",
    "    cid_list = []\n",
    "    if os.path.exists(target_path):\n",
    "        cid_list = traverse_sub_node(target_path)\n",
    "    if os.path.exists(target_path+'.txt'):\n",
    "        cid_list = process_leaf_node(target_path+'.txt')\n",
    "    return cid_list\n",
    "\n",
    "def ipfs_get_feature(cid):\n",
    "    import subprocess\n",
    "    subprocess.check_output([\"ipfs\", \"get\", cid])\n",
    "    return gpd.read_file(f\"./{cid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5261958-a433-47ef-98a9-6f91cf7aafcc",
   "metadata": {},
   "source": [
    "### Benchmark for ipfs+geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a597a37-e5f4-4100-862d-74bbdbd44e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "import subprocess\n",
    "subprocess.check_output([\"ipfs\", \"get\", \"-o\",\"../data/test/dc\",\"QmPaXWva3WQR2uwFdu6bkizUyRyKpX3V1aiT5dvnnYKSpJ\"])\n",
    "#query\n",
    "results = multi_geohash_query(rn,f\"../data/test/dc\")\n",
    "os.chdir(\"../data/test/\")\n",
    "#ipfs retrieval\n",
    "ipfs_retrieval = pd.concat([ipfs_get_feature(cid) for cid in results])\n",
    "os.chdir(\"../../notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef1c7e-5d44-4250-8b14-80946c6224af",
   "metadata": {},
   "source": [
    "### Benchmark for local I/O+geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5c315-7623-4f36-a41c-de84c7536ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#query\n",
    "results = multi_geohash_query(rn,f\"../data/geohash_{asset}/index\")\n",
    "local_io_retrieval = pd.concat([gpd.read_file(path) for path in gdf[gdf.single_cid.isin(results)].single_path.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f312e-3eb7-4a17-a724-6d0886aea518",
   "metadata": {},
   "source": [
    "### Benchmark for geopandas query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8163e-e0d9-47b4-8a98-931fa0968176",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#candidate_points = gpd.read_file(f\"../../data/maryland_demo/{asset}_cid.geojson\")\n",
    "gpd.sjoin(candidate_points, neighbors, how=\"inner\", op=\"within\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc508c36-f165-4eeb-b9f4-2b3507f42cf9",
   "metadata": {},
   "source": [
    "### Benchmark for postgresql\n",
    "run docker daemon and the command to spin up postgresql\n",
    "`docker-compose up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e3506-1c99-4150-9ed4-559132e3a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "# Load GeoJSON into a GeoDataFrame\n",
    "gdf = gpd.read_file(f\"../../data/maryland_demo/{asset}.geojson\")\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "engine = create_engine('postgresql://user:password@localhost:5432/geodb')\n",
    "\n",
    "# Load data into PostgreSQL\n",
    "gdf.to_postgis(f'{asset}', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ce8c4-a7da-428d-977e-d2db5dea5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create spatial index\n",
    "# Parameters for connection\n",
    "params = {\n",
    "    'dbname': 'geodb',\n",
    "    'user': 'user',\n",
    "    'password': 'password',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = psycopg2.connect(**params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the CREATE INDEX command\n",
    "cur.execute(f'CREATE INDEX ON {asset} USING gist(geometry)')\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef6dbf-f53e-4b97-9367-f651c92a2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store query target geodataframe into database\n",
    "neighbors.crs = \"EPSG:4326\"\n",
    "neighbors.to_postgis(\"temp_table\", engine, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67e9c7-14af-4a44-afa5-4c6e69113ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark\n",
    "\n",
    "%%timeit\n",
    "conn = psycopg2.connect(**params)\n",
    "# Create a new cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute a COUNT SQL\n",
    "sql = f\"\"\"\n",
    "SELECT COUNT(*) FROM \n",
    "\n",
    "(SELECT {asset}.geometry\n",
    "FROM {asset}, temp_table\n",
    "WHERE ST_Intersects({asset}.geometry, temp_table.geometry)) AS R;\n",
    "\"\"\"\n",
    "cur.execute(sql)\n",
    "\n",
    "# Fetch the result\n",
    "count = cur.fetchone()[0]\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cda261-d8bb-436a-93e6-d8c68ef0508b",
   "metadata": {},
   "source": [
    "### Benchmark comparison\n",
    "\n",
    "| Query Method for queen neighbor proximity | Number of nearby features <br> (per loop time of 7 runs, 10 loops each) | Retrieval of nearby features| \n",
    "|----------|----------|----------|\n",
    "| Geohash + local I/O | 0.77 ms   |3190 ms |\n",
    "| Geohash + IPFS | 257 ms|13500 ms|\n",
    "| In-memory query (geopandas.sjoin ) | 20 ms | 14600 ms|\n",
    "| Postgis | 21.3 ms |18.8 ms|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddef34-1a82-41f1-97e8-ae6e953fad58",
   "metadata": {},
   "source": [
    "From the results of local I/O, we can see pre-calculated geohash can significant reduce the query time at the specific task. However, IPFS online test indicates the I/O expense is still under optimized for IPFS system when retrieving the index file is inevitable. When including the file retrieval, partial retrieval with geohash overperformed the in-memory query where geohash is not available and all the assets need to be loaded into the memory. Yet local database system is still the optimized option when considering the retrieval process implying improvment in file exchange process within IPFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700406c-e483-43f8-a11a-0b960b95e26d",
   "metadata": {},
   "source": [
    "## Try yourself\n",
    "To replicate the code locally, you may check out the [project repository](https://github.com/leonardzh/geohash-cid) for environment setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac76efc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
